import os
import glob
from src.pipelines.ingest_pipeline import ingest_contract
from src.indexer.search_indexer import index_chunks, delete_all_documents

INPUT_DIR = "inputdocs"
PROCESSED_DIR = "processed"


def ensure_inputdocs_folder():
    """Create inputdocs/ folder if it doesn't exist."""
    if not os.path.exists(INPUT_DIR):
        print("ğŸ“ Creating inputdocs/ folder...")
        os.makedirs(INPUT_DIR)
        print("âœ” inputdocs/ folder created. Add PDF files and rerun.")
    else:
        print("ğŸ“ inputdocs/ folder already exists.")


def clean_processed_folder():
    """Delete processed JSONL files."""
    if not os.path.exists(PROCESSED_DIR):
        os.makedirs(PROCESSED_DIR)
        return

    for f in glob.glob(os.path.join(PROCESSED_DIR, "*.jsonl")):
        print(f"ğŸ—‘ Removing: {f}")
        os.remove(f)


def get_input_files():
    """Return list of PDFs in inputdocs."""
    pdfs = glob.glob(os.path.join(INPUT_DIR, "*.pdf"))
    if not pdfs:
        print("âš  No PDF files inside inputdocs/. Add files and rerun.")
    return pdfs


def index_all_documents():

    print("\n=========================")
    print("   RAG INDEXING SYSTEM")
    print("=========================\n")

    ensure_inputdocs_folder()
    clean_processed_folder()

    print("\nğŸ§¹ Flushing Azure Search index...")
    delete_all_documents()

    pdf_files = get_input_files()
    if not pdf_files:
        return

    for pdf in pdf_files:
        print(f"\nğŸ“„ Processing: {pdf}")
        result = ingest_contract(pdf, model="prebuilt-read")

        doc_id = result["doc_id"]
        chunks = result["chunks"]

        print(f"âœ” Ingested doc id: {doc_id}")
        print(f"âœ” Total chunks: {len(chunks)}")

        print("ğŸ“¤ Uploading chunks to Azure Searchâ€¦")
        index_chunks(chunks)

        print(f"âœ” Finished indexing {pdf}")

    print("\nğŸ‰ All documents indexed successfully!")


if __name__ == "__main__":
    index_all_documents()
